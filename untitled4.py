# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rJmX6hl3nyf5rjDrAufkvoPzW71mnWsA
"""

# ==========================
# ADVANCED TIME SERIES FORECASTING WITH TRANSFORMER ATTENTION
# ==========================

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error
import math
import tensorflow as tf
from tensorflow.keras.layers import LayerNormalization, MultiHeadAttention, Layer, Dense, LSTM, Input, Dropout
from tensorflow.keras.models import Sequential, Model
from statsmodels.tsa.statespace.sarimax import SARIMAX

# =====================
# 1. Synthetic multivariate dataset generation
# =====================
def generate_data(n_points=1500):
    time = np.arange(n_points)
    feature1 = np.sin(0.02 * time) + np.random.normal(scale=0.1, size=n_points)
    feature2 = np.cos(0.02 * time) + np.random.normal(scale=0.1, size=n_points)
    trend = time * 0.001
    target = feature1 * 0.5 + feature2 * 0.3 + trend
    df = pd.DataFrame({"feature1": feature1, "feature2": feature2, "target": target})
    return df

df = generate_data()
print(df.head())

# =====================
# 2. Preprocessing & Windowing
# =====================
scaler = MinMaxScaler()
scaled = scaler.fit_transform(df)

def create_windows(data, win=30, horizon=1):
    X, y = [], []
    for i in range(len(data) - win - horizon):
        X.append(data[i:i + win, :])
        y.append(data[i + win + horizon - 1, -1])
    return np.array(X), np.array(y)

window_size = 30
X, y = create_windows(scaled)

split = int(len(X) * 0.7)
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

# =====================
# 3. Baseline SARIMA Model
# =====================
sarima_train = df["target"][:split]
sarima_test = df["target"][split:]

sarima = SARIMAX(sarima_train, order=(3, 1, 2), seasonal_order=(1, 1, 1, 12)).fit()
sarima_pred = sarima.forecast(len(sarima_test))

mae_sarima = mean_absolute_error(sarima_test, sarima_pred)
rmse_sarima = math.sqrt(mean_squared_error(sarima_test, sarima_pred))
print("SARIMA MAE:", mae_sarima, "RMSE:", rmse_sarima)

# =====================
# 4. Baseline LSTM Model
# =====================
lstm = Sequential([
    LSTM(64, return_sequences=False, input_shape=(window_size, X.shape[2])),
    Dense(1)
])

lstm.compile(loss='mse', optimizer='adam')
lstm.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)

pred_lstm = lstm.predict(X_test)
mae_lstm = mean_absolute_error(y_test, pred_lstm)
rmse_lstm = math.sqrt(mean_squared_error(y_test, pred_lstm))
print("LSTM MAE:", mae_lstm, "RMSE:", rmse_lstm)

# =====================
# 5. Transformer Attention Model
# =====================

def transformer_model():
    inputs = Input(shape=(window_size, X.shape[2]))
    attention = MultiHeadAttention(num_heads=4, key_dim=16)(inputs, inputs)
    attention = LayerNormalization()(attention + inputs)
    dense = Dense(32, activation='relu')(attention)
    dense = Dropout(0.2)(dense)
    output = Dense(1)(dense[:, -1, :])
    model = Model(inputs, output)
    return model

transformer = transformer_model()
transformer.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(0.0005))
history = transformer.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.1)

pred_trans = transformer.predict(X_test)
mae_trans = mean_absolute_error(y_test, pred_trans)
rmse_trans = math.sqrt(mean_squared_error(y_test, pred_trans))
print("Transformer MAE:", mae_trans, "RMSE:", rmse_trans)

# =====================
# 6. Attention Weight Extraction
# =====================
extract_attention = Model(inputs=transformer.input, outputs=transformer.layers[1].output)
att_weights = extract_attention.predict(X_test[:1])

plt.imshow(att_weights[0], aspect='auto')
plt.title("Attention Weights Heatmap")
plt.colorbar()
plt.show()

# =====================
# 7. Comparison output
# =====================
print("\n==== MODEL PERFORMANCE COMPARISON ====")
print(f"SARIMA:    MAE={mae_sarima:.4f}, RMSE={rmse_sarima:.4f}")
print(f"LSTM:      MAE={mae_lstm:.4f}, RMSE={rmse_lstm:.4f}")
print(f"Transformer MAE={mae_trans:.4f}, RMSE={rmse_trans:.4f}")